# TinyGPT-V

<font size='5'>**TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones**</font>

Zhengqing Yuanâ, Zhaoxu Liâ, Lichao Sunâ‹

âVisiting Students at LAIR Lab, Lehigh University
â‹Lehigh University

</a> <a href='https://arxiv.org/abs/2312.16862'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>  <a href='https://huggingface.co/Tyrannosaurus/TinyGPT-V'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue'></a> <a href='https://huggingface.co/spaces/llizhx/TinyGPT-V'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'> 

[English](/README.md) | ç®€ä½“ä¸­æ–‡
</font>

## æœ€æ–°æ¶ˆæ¯
[Jan.03 2024] æˆ‘ä»¬åˆ›å»ºäº†huggingface demoï¼Œè¯•ç”¨æˆ‘ä»¬çš„æ¨¡å‹(ç¬¬ä¸‰é˜¶æ®µ)!

[Dec.28 2023] æˆ‘ä»¬å…¬å¼€äº†TinyGPT-Vçš„ä»£ç .

## TinyGPT-V è®­ç»ƒè¿‡ç¨‹
![Traning_Process](examples/Training_S.png)

## TinyGPT-V æ¨¡å‹ç»“æ„
![Model](examples/TinyGPT-V-ST.png)

## TinyGPT-V ç»“æœ
![Results](examples/result.png)





## å‡†å¤‡å¼€å§‹
### ä¸‹è½½

**1. å‡†å¤‡ä»£ç å’Œç¯å¢ƒ**

Gitå…‹éš†æˆ‘ä»¬çš„å­˜å‚¨åº“ï¼Œåˆ›å»ºä¸€ä¸ªpythonç¯å¢ƒï¼Œå¹¶é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¿€æ´»å®ƒ:

```bash
git clone https://github.com/DLYuanGod/TinyGPT-V.git
cd TinyGPT-V
conda env create -f environment.yml
conda activate tinygptv
```


**2. å‡†å¤‡é¢„è®­ç»ƒçš„LLMæƒé‡**

**TinyGPT-V** åŸºäºPhi-2. 
ä»ä¸‹é¢çš„huggingfaceç©ºé—´ä¸‹è½½ç›¸åº”çš„LLMæƒé‡é€šè¿‡git-lfså…‹éš†å­˜å‚¨åº“:

Phi-2 2.7B: [Download](https://huggingface.co/susnato/phi-2)


ç„¶åï¼Œå°†æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­çš„å˜é‡*phi_model*è®¾ç½®ä¸ºLLMæƒé‡è·¯å¾„ã€‚

* è®¾ç½®LLMè·¯å¾„ [here](minigpt4/configs/models/minigpt_v2.yaml#L14) åœ¨ç¬¬14è¡Œ, [here](minigpt4/configs/models/minigpt4_vicuna0.yaml#L18) ç¬¬18è¡Œ [here](minigpt4/conversation/conversation.py#L16) ç¬¬16è¡Œ.





**3. å‡†å¤‡é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡**

ä¸‹è½½é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡*

| é˜¶æ®µ1å | é˜¶æ®µ2å | é˜¶æ®µ3å | é˜¶æ®µ4å | 
| ------ | ------ | ------ | -------|
| [Download](https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage1.pth) |[Download](https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage2.pth) | [Download](https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage3.pth) |[Download](https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage4.pth) |


åœ¨è¯„ä¼°é…ç½®æ–‡ä»¶ä¸­ç»™**TinyGPT-V**è®¾ç½®é¢„è®­ç»ƒæƒé‡çš„è·¯å¾„

é˜¶æ®µ1ï¼Œ2ï¼Œ3ï¼š[tinygptv_stage1_2_3_eval.yaml](eval_configs/tinygptv_stage1_2_3_eval.yaml#L8) ï¼Œæˆ–è€…é˜¶æ®µ4ï¼š[tinygptv_stage4_eval.yaml](eval_configs/tinygptv_stage4_eval.yaml#L8) çš„ç¬¬8è¡Œ.   


**4. æ›´æ–°transformersåº“çš„Phi-2æ¨¡å‹.**

Linuxç³»ç»Ÿ:

```
cp modeling_phi.py /root/miniconda3/envs/tinygptv/lib/python3.9/site-packages/transformers/models/phi/
```

Windowsç³»ç»Ÿ 

æ‰¾åˆ°ä½ è‡ªå·±çš„: conda_sit/envs/tinygptv/lib/python3.9/site-packages/transformers/models/phi/ ç„¶åç”¨TinyGPT-V/modeling_phi.py æ›¿æ¢ modeling_phi.py .


### åœ¨æœ¬åœ°åˆ›å»ºdemo

å¯¹äºé˜¶æ®µ4, è¿è¡Œ

```
python demo_v2.py --cfg-path eval_configs/tinygptv_stage4_eval.yaml  --gpu-id 0
```

å¯¹äºé˜¶æ®µ1ï¼Œ2ï¼Œ3, è¿è¡Œ
```
python demo.py --cfg-path eval_configs/tinygptv_stage1_2_3_eval.yaml  --gpu-id 0
```


ä¸ºäº†ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹ï¼ŒLLMé»˜è®¤åŠ è½½ä¸º16ä½ã€‚æ­¤é…ç½®å¤§çº¦éœ€è¦8G GPUå†…å­˜ã€‚ä¸ºäº†æ›´èŠ‚çœGPUå†…å­˜ï¼Œä½ å¯ä»¥é€šè¿‡åœ¨ç›¸å…³é…ç½®æ–‡ä»¶ä¸­è®¾ç½®â€œlow_resourceâ€ä¸ºâ€œTrueâ€æ¥ä»¥8ä½åœ¨8Gä»¥ä¸‹çš„è®¾å¤‡è¿è¡Œ:

* é˜¶æ®µ4 [tinygptv_stage4_eval.yaml](eval_configs/tinygptv_stage4_eval.yaml#6) 

* é˜¶æ®µ1ï¼Œ2ï¼Œ3 [tinygptv_stage1_2_3_eval.yaml](eval_configs/tinygptv_stage1_2_3_eval.yaml#6) 


```diff
-æ³¨:ç¬¬4é˜¶æ®µç›®å‰æ˜¯æµ‹è¯•ç‰ˆæœ¬ï¼Œå› ä¸ºå®ƒä½¿ç”¨éƒ¨åˆ†æ•°æ®è¿›è¡Œè®­ç»ƒã€‚è¯·ä½¿ç”¨ç¬¬3é˜¶æ®µè¿›è¡Œæ¼”ç¤ºã€‚
```

### Tè®­ç»ƒ

é¦–å…ˆï¼Œæ‚¨éœ€è¦è°ƒæ•´LLMä¸­æ‰€æœ‰æ›´æ–°çš„æƒé‡ï¼Œä»¥ä¾¿ä»¥å…¨ç²¾åº¦è®¡ç®—ï¼š[Here](minigpt4\models\base_model.py). åˆ é™¤ä»¥ä¸‹è¡Œä¸­çš„æ³¨é‡Š:

```
                layer.self_attn.q_layernorm.weight.data = layer.self_attn.q_layernorm.weight.data.float()
                layer.self_attn.k_layernorm.weight.data = layer.self_attn.k_layernorm.weight.data.float()
                layer.post_layernorm.weight.data = layer.post_layernorm.weight.data.float()
                layer.input_layernorm.weight.data = layer.input_layernorm.weight.data.float()

                # Perform a similar operation for the bias item
                if layer.self_attn.q_layernorm.bias is not None:
                    layer.self_attn.q_layernorm.bias.data = layer.self_attn.q_layernorm.bias.data.float()
                if layer.self_attn.k_layernorm.bias is not None:
                    layer.self_attn.k_layernorm.bias.data = layer.self_attn.k_layernorm.bias.data.float()
                if layer.input_layernorm.bias is not None:
                    layer.input_layernorm.bias.data = layer.input_layernorm.bias.data.float()


            llama_model.model.model.final_layernorm.weight.requires_grad = True
            llama_model.model.model.final_layernorm.weight.data = llama_model.model.model.final_layernorm.weight.data.float()
            if llama_model.model.model.final_layernorm.bias is not None:
                llama_model.model.model.final_layernorm.bias.data = llama_model.model.model.final_layernorm.bias.float()
```

**é˜¶æ®µ1ï¼Œ2:**

* æ•°æ®é›†: [first stage dataset preparation instruction](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_1_STAGE.md)

* ç„¶åè¿è¡Œ:
```
torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage1.yaml
```
æ‚¨éœ€è¦æ‰§è¡Œä¸Šè¿°ä»£ç 17æ¬¡æ‰èƒ½å®Œæˆç¬¬ä¸€é˜¶æ®µçš„åŸ¹è®­ã€‚

* ç„¶åè¿è¡Œ:
```
torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage2.yaml
```

**é˜¶æ®µ3:**

* æ•°æ®é›†: [stage 3 dataset preparation instruction](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_2_STAGE.md)

* ç„¶åè¿è¡Œ:
```
torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage3.yaml
```

**é˜¶æ®µ4:**

* æ•°æ®é›†: [stage 4 dataset preparation instruction](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_MINIGPTv2_FINETUNE.md) è¯·å‡†å¤‡æ‰€æœ‰æ•°æ®é›†é™¤äº† COCO captions å’Œ OCR-VQA.

* ç„¶åè¿è¡Œ:
```
torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage4.yaml
```

### è¯„ä¼°
æŸ¥çœ‹TinyGPT-Vçš„è¯„ä¼°è¯¦ç»†ä¿¡æ¯ [here](eval_scripts/EVAL_README.md)  



## åŠ ğŸŒŸå†å²

[![Star History Chart](https://api.star-history.com/svg?repos=DLYuanGod/TinyGPT-V&type=Date)](https://star-history.com/#DLYuanGod/TinyGPT-V&Date)


## è‡´è°¢

+ [MiniGPT](https://github.com/Vision-CAIR/MiniGPT-4) ä¸€ä¸ªéå¸¸é€šç”¨çš„MLLMs.


å¦‚æœæ‚¨åœ¨æ‚¨çš„ç ”ç©¶æˆ–åº”ç”¨ä¸­ä½¿ç”¨TinyGPT-Vï¼Œè¯·ä½¿ç”¨æœ¬BibTeXå¼•ç”¨ï¼š
```bibtex

@misc{yuan2023tinygptv,
      title={TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones}, 
      author={Zhengqing Yuan and Zhaoxu Li and Lichao Sun},
      year={2023},
      eprint={2312.16862},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


## è®¸å¯è¯
è¯¥é¡¹ç›®å¼€æºè‡ª [BSD 3-Clause License](LICENSE.md).
æˆ‘ä»¬çš„ä»£ç åŸºäº [Lavis](https://github.com/salesforce/LAVIS) ä¸ 
BSD 3-Clause è®¸å¯è¯ [here](LICENSE_Lavis.md).